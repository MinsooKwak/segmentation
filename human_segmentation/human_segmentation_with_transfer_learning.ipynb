{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOuppJs5IdEU3h4j44PAAE/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MinsooKwak/segmentation/blob/main/human_segmentation/human_segmentation_with_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- kaggle의 Human Segmentation Dataset을 활용\n",
        "  - 링크 : https://www.kaggle.com/datasets/tapakah68/supervisely-filtered-segmentation-person-dataset/data\n",
        "  - 사람에 대한 마스크 기반 segmentation"
      ],
      "metadata": {
        "id": "0_bFD1BmC0r4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GBgRyhhGCr57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b77c3d65-8512-4020-eb0d-4c0d36601fbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2"
      ],
      "metadata": {
        "id": "rKgERyReXVm0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 데이터셋"
      ],
      "metadata": {
        "id": "uvJNT81HYWpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/DataSet/human_segmentation/'\n",
        "data_df = pd.read_csv(os.path.join(data_dir, \"df.csv\"))\n",
        "data_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "DKaJ91QqXiu1",
        "outputId": "022d3847-40b6-4f7a-94cc-faeca5169c13"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                               images  \\\n",
              "0           0  images/ds10_pexels-photo-687782.png   \n",
              "1           1  images/ds10_pexels-photo-835971.png   \n",
              "2           2  images/ds10_pexels-photo-850708.png   \n",
              "3           3  images/ds10_pexels-photo-864937.png   \n",
              "4           4  images/ds10_pexels-photo-865908.png   \n",
              "\n",
              "                                masks                              collages  \n",
              "0  masks/ds10_pexels-photo-687782.png  collage/ds10_pexels-photo-687782.jpg  \n",
              "1  masks/ds10_pexels-photo-835971.png  collage/ds10_pexels-photo-835971.jpg  \n",
              "2  masks/ds10_pexels-photo-850708.png  collage/ds10_pexels-photo-850708.jpg  \n",
              "3  masks/ds10_pexels-photo-864937.png  collage/ds10_pexels-photo-864937.jpg  \n",
              "4  masks/ds10_pexels-photo-865908.png  collage/ds10_pexels-photo-865908.jpg  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a9cbf16d-a65e-4bb6-ab4e-15eddcacd7d4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>images</th>\n",
              "      <th>masks</th>\n",
              "      <th>collages</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>images/ds10_pexels-photo-687782.png</td>\n",
              "      <td>masks/ds10_pexels-photo-687782.png</td>\n",
              "      <td>collage/ds10_pexels-photo-687782.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>images/ds10_pexels-photo-835971.png</td>\n",
              "      <td>masks/ds10_pexels-photo-835971.png</td>\n",
              "      <td>collage/ds10_pexels-photo-835971.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>images/ds10_pexels-photo-850708.png</td>\n",
              "      <td>masks/ds10_pexels-photo-850708.png</td>\n",
              "      <td>collage/ds10_pexels-photo-850708.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>images/ds10_pexels-photo-864937.png</td>\n",
              "      <td>masks/ds10_pexels-photo-864937.png</td>\n",
              "      <td>collage/ds10_pexels-photo-864937.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>images/ds10_pexels-photo-865908.png</td>\n",
              "      <td>masks/ds10_pexels-photo-865908.png</td>\n",
              "      <td>collage/ds10_pexels-photo-865908.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a9cbf16d-a65e-4bb6-ab4e-15eddcacd7d4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a9cbf16d-a65e-4bb6-ab4e-15eddcacd7d4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a9cbf16d-a65e-4bb6-ab4e-15eddcacd7d4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-49dd795b-90dc-4164-861c-c3129a32ad94\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-49dd795b-90dc-4164-861c-c3129a32ad94')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-49dd795b-90dc-4164-861c-c3129a32ad94 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = data_df[['images','masks']]\n",
        "df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "Ur7U_vYJX3Qi",
        "outputId": "25daaa66-122d-4ccd-b28a-6a9a18bb68d3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                images                               masks\n",
              "0  images/ds10_pexels-photo-687782.png  masks/ds10_pexels-photo-687782.png\n",
              "1  images/ds10_pexels-photo-835971.png  masks/ds10_pexels-photo-835971.png\n",
              "2  images/ds10_pexels-photo-850708.png  masks/ds10_pexels-photo-850708.png"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7f206fa4-a4e3-4e1b-a4e2-e0c16a160bbf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>images</th>\n",
              "      <th>masks</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>images/ds10_pexels-photo-687782.png</td>\n",
              "      <td>masks/ds10_pexels-photo-687782.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>images/ds10_pexels-photo-835971.png</td>\n",
              "      <td>masks/ds10_pexels-photo-835971.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>images/ds10_pexels-photo-850708.png</td>\n",
              "      <td>masks/ds10_pexels-photo-850708.png</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f206fa4-a4e3-4e1b-a4e2-e0c16a160bbf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7f206fa4-a4e3-4e1b-a4e2-e0c16a160bbf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7f206fa4-a4e3-4e1b-a4e2-e0c16a160bbf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4b238fb8-4fc3-4050-8337-c563166fcb2c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4b238fb8-4fc3-4050-8337-c563166fcb2c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4b238fb8-4fc3-4050-8337-c563166fcb2c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터셋 구축 및 텐서 변환 모듈 작성"
      ],
      "metadata": {
        "id": "kHrPmVDp_pxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "jmUqyvWB-EOt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/DataSet/human_segmentation/'\n",
        "data_dir2 = \"/content/drive/MyDrive/DataSet/human_segmentation_split\""
      ],
      "metadata": {
        "id": "Fz9eHoBHaco7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Transfer learning을 위해 일부 데이터셋만 가져와 구성\n",
        "  - Image, Mask에 대해 train, val set 250씩 구성"
      ],
      "metadata": {
        "id": "DjRMJjj51THi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install split-folders[full]"
      ],
      "metadata": {
        "id": "jiw6rgTALYdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e5fd2d4-57f9-48a5-fcba-e13be5b88f19"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting split-folders[full]\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from split-folders[full]) (4.66.1)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import splitfolders"
      ],
      "metadata": {
        "id": "kb01h5qfMCMJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 고정된 개수 데이터로 나눔 (train 250, val 250)\n",
        "splitfolders.fixed(data_dir, output=data_dir2, seed=2024, fixed=(250,250))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xL5mnWV7LjZM",
        "outputId": "4c4d422d-6eed-4461-d839-81c2fe684cd7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 8001 files [20:03,  6.65 files/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "f4hKSrl8WlQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "IMAGE_SIZE = 224"
      ],
      "metadata": {
        "id": "7xcviYobWmyf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class human_dataset():\n",
        "  def __init__(self,data_dir, phase, transformer=None):\n",
        "    self.phase = phase\n",
        "    self.images_dir = os.path.join(data_dir, phase, \"images\")\n",
        "    self.masks_dir = os.path.join(data_dir, phase, \"masks\")\n",
        "    self.image_files = [filename for filename in os.listdir(self.images_dir) if filename.endswith('png')]\n",
        "    self.mask_files = [filename for filename in os.listdir(self.masks_dir) if filename.endswith('png')]\n",
        "    assert len(self.image_files) == len(self.mask_files)\n",
        "\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_files)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    image = cv2.imread(os.path.join(self.images_dir, self.image_files[index]))\n",
        "    image = cv2.resize(image, dsize=(IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_LINEAR)\n",
        "    mask = cv2.imread(os.path.join(self.masks_dir, self.mask_files[index]))\n",
        "    mask = cv2.resize(mask, dsize=(IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # 정리 (JPEG 같은 경우)\n",
        "    mask[mask < 240] = 0\n",
        "    mask[mask >=240] = 255\n",
        "    mask = mask/255.\n",
        "\n",
        "    mask_H, mask_W, mask_C = mask.shape\n",
        "    background = np.ones(shape = (mask_H, mask_W))\n",
        "    background[mask[...,0]!= 0] = 0\n",
        "    background[mask[...,1]!= 0] = 0\n",
        "    background[mask[...,2]!= 0] = 0\n",
        "\n",
        "    mask = np.concatenate([np.expand_dims(background, axis=-1), mask], axis=-1)\n",
        "    mask = np.argmax(mask, axis=-1, keepdims=False)\n",
        "\n",
        "    if self.transformer:\n",
        "      image = self.transformer(image)\n",
        "\n",
        "    target = torch.from_numpy(mask).long()\n",
        "    return image, target"
      ],
      "metadata": {
        "id": "UgzABrs508cr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Xx01Xw3oizlD",
        "outputId": "ec999ddc-71e3-4991-a2a0-8aed7960208e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/DataSet/human_segmentation_split'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dset = human_dataset(data_dir2, \"train\")"
      ],
      "metadata": {
        "id": "uJ4k2FIEmSJE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dset[0] # numpy 타입의 image data / tensor형의 target 값 전달"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqkp3QWwmURd",
        "outputId": "b28eaaa9-e199-469a-f6a1-5d427fa52698"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[[ 40,  28,  28],\n",
              "         [ 43,  30,  32],\n",
              "         [ 37,  24,  26],\n",
              "         ...,\n",
              "         [180, 183, 168],\n",
              "         [186, 189, 174],\n",
              "         [186, 189, 174]],\n",
              " \n",
              "        [[ 41,  28,  30],\n",
              "         [ 39,  27,  28],\n",
              "         [ 38,  25,  27],\n",
              "         ...,\n",
              "         [178, 181, 165],\n",
              "         [183, 186, 171],\n",
              "         [188, 191, 176]],\n",
              " \n",
              "        [[ 44,  31,  33],\n",
              "         [ 40,  27,  29],\n",
              "         [ 43,  30,  32],\n",
              "         ...,\n",
              "         [177, 180, 164],\n",
              "         [184, 187, 171],\n",
              "         [189, 192, 177]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[103, 112, 125],\n",
              "         [103, 114, 128],\n",
              "         [102, 113, 127],\n",
              "         ...,\n",
              "         [132, 156, 154],\n",
              "         [175, 187, 193],\n",
              "         [156, 175, 188]],\n",
              " \n",
              "        [[ 99, 108, 121],\n",
              "         [102, 111, 124],\n",
              "         [ 90,  99, 113],\n",
              "         ...,\n",
              "         [118, 138, 133],\n",
              "         [162, 174, 184],\n",
              "         [145, 164, 178]],\n",
              " \n",
              "        [[101, 107, 120],\n",
              "         [ 97, 107, 120],\n",
              "         [ 97, 106, 120],\n",
              "         ...,\n",
              "         [ 74,  88,  87],\n",
              "         [111, 126, 128],\n",
              "         [ 94, 113, 119]]], dtype=uint8),\n",
              " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0]]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- transformer 작성"
      ],
      "metadata": {
        "id": "0yqKvIkajTfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "HnKb8De3mYHb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer():\n",
        "  transformer = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
        "  ])\n",
        "  return transformer"
      ],
      "metadata": {
        "id": "gUbvbXPImZcf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = build_transformer()\n",
        "dset = human_dataset(data_dir = data_dir2, phase='train', transformer = transformer)"
      ],
      "metadata": {
        "id": "VKKHGeIpmbx4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxsvSg1VlQuT",
        "outputId": "89030562-cdd4-4562-cca1-28734c9d74ea"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[-1.4329, -1.3815, -1.4843,  ...,  0.9646,  1.0673,  1.0673],\n",
              "          [-1.4158, -1.4500, -1.4672,  ...,  0.9303,  1.0159,  1.1015],\n",
              "          [-1.3644, -1.4329, -1.3815,  ...,  0.9132,  1.0331,  1.1187],\n",
              "          ...,\n",
              "          [-0.3541, -0.3541, -0.3712,  ...,  0.1426,  0.8789,  0.5536],\n",
              "          [-0.4226, -0.3712, -0.5767,  ..., -0.0972,  0.6563,  0.3652],\n",
              "          [-0.3883, -0.4568, -0.4568,  ..., -0.8507, -0.2171, -0.5082]],\n",
              " \n",
              "         [[-1.5455, -1.5105, -1.6155,  ...,  1.1681,  1.2731,  1.2731],\n",
              "          [-1.5455, -1.5630, -1.5980,  ...,  1.1331,  1.2206,  1.3081],\n",
              "          [-1.4930, -1.5630, -1.5105,  ...,  1.1155,  1.2381,  1.3256],\n",
              "          ...,\n",
              "          [-0.0749, -0.0399, -0.0574,  ...,  0.6954,  1.2381,  1.0280],\n",
              "          [-0.1450, -0.0924, -0.3025,  ...,  0.3803,  1.0105,  0.8354],\n",
              "          [-0.1625, -0.1625, -0.1800,  ..., -0.4951,  0.1702, -0.0574]],\n",
              " \n",
              "         [[-1.3164, -1.2467, -1.3513,  ...,  1.1237,  1.2282,  1.2282],\n",
              "          [-1.2816, -1.3164, -1.3339,  ...,  1.0714,  1.1759,  1.2631],\n",
              "          [-1.2293, -1.2990, -1.2467,  ...,  1.0539,  1.1759,  1.2805],\n",
              "          ...,\n",
              "          [ 0.3742,  0.4265,  0.4091,  ...,  0.8797,  1.5594,  1.4722],\n",
              "          [ 0.3045,  0.3568,  0.1651,  ...,  0.5136,  1.4025,  1.2980],\n",
              "          [ 0.2871,  0.2871,  0.2871,  ..., -0.2881,  0.4265,  0.2696]]]),\n",
              " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0]]))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- dataloader에서 데이터를 반복적으로 불러오기 위한 함수"
      ],
      "metadata": {
        "id": "iQjcnOoJlT4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  images = []\n",
        "  targets = []\n",
        "  for a, b in batch:\n",
        "    images.append(a)\n",
        "    targets.append(b)\n",
        "  images = torch.stack(images, dim=0)\n",
        "  targets = torch.stack(targets, dim=0)\n",
        "  return images, targets"
      ],
      "metadata": {
        "id": "4L-BKZIglR2t"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "rR2CKnKHlnYT"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dloader = DataLoader(dset, batch_size=4, shuffle=True, collate_fn = collate_fn)"
      ],
      "metadata": {
        "id": "mjPR2Bw0lqJk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터로더 동작 확인"
      ],
      "metadata": {
        "id": "s-3FWjIGmIVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, batch in enumerate(dloader):\n",
        "  images = batch[0]\n",
        "  targets = batch[1]\n",
        "\n",
        "  print(f'images shape : {images.shape}')\n",
        "  print(f'targets shape : {targets.shape}')\n",
        "  if index ==0:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unLggJjPlqEF",
        "outputId": "60627fd9-42e4-431d-feae-c0b882b9a6ac"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images shape : torch.Size([4, 3, 224, 224])\n",
            "targets shape : torch.Size([4, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터로더 빌드"
      ],
      "metadata": {
        "id": "JyeaO4BKmh02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataloader(data_dir, batch_size=4):\n",
        "  transformer = build_transformer()\n",
        "\n",
        "  dataloaders = {}\n",
        "  train_dataset = human_dataset(data_dir=data_dir2, phase='train', transformer=transformer)\n",
        "  dataloaders['train'] = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "  val_dataset = human_dataset(data_dir=data_dir2, phase='val', transformer=transformer)\n",
        "  dataloaders['val'] = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "  return dataloaders"
      ],
      "metadata": {
        "id": "6RTNJKx6mjd1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터로더 확인"
      ],
      "metadata": {
        "id": "8lg_kHybnxS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = build_dataloader(data_dir=data_dir2, batch_size=4)\n",
        "\n",
        "for phase in ['train','val']:\n",
        "  for index, batch in enumerate(dataloaders[phase]):\n",
        "    images = batch[0]\n",
        "    targets = batch[1]\n",
        "    print(f'images shape : {images.shape}')\n",
        "    print(f'masks shape : {targets.shape}')\n",
        "\n",
        "    if index ==0:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7P2IPBAni15",
        "outputId": "91f08b1a-8b9d-4309-843d-a6248f2062cd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images shape : torch.Size([4, 3, 224, 224])\n",
            "masks shape : torch.Size([4, 224, 224])\n",
            "images shape : torch.Size([4, 3, 224, 224])\n",
            "masks shape : torch.Size([4, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VGG16 Backbone UNet architecture"
      ],
      "metadata": {
        "id": "-e4l_MJZokPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "DNFH64Ivo9Tu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ConvLayer(in_channels, out_channels, kernel_size=3, padding=1):\n",
        "  layers = nn.Sequential(\n",
        "      nn.Conv2d(in_channels, out_channels, kernel_size= kernel_size, padding=padding),\n",
        "      nn.BatchNorm2d(out_channels),\n",
        "      nn.ReLU(inplace=True),\n",
        "\n",
        "      nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
        "      nn.BatchNorm2d(out_channels),\n",
        "      nn.ReLU(inplace=True),\n",
        "  )\n",
        "  return layers"
      ],
      "metadata": {
        "id": "STDzG1UconYt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def UpConvLayer(in_channels, out_channels):\n",
        "  layers = nn.Sequential(\n",
        "      nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
        "      nn.BatchNorm2d(out_channels),\n",
        "      nn.ReLU(inplace=True)\n",
        "  )\n",
        "  return layers"
      ],
      "metadata": {
        "id": "Q9OdHeh5pdw5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models"
      ],
      "metadata": {
        "id": "w-ZiS5hHpukS"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델 구조 보고 어디까지 쓸지 먼저 파악"
      ],
      "metadata": {
        "id": "hekgi024qdH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16 = models.vgg16_bn(pretrained=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nl0Fl23pxXN",
        "outputId": "4bb2f07d-a56e-4e82-b45e-2abeab65b7ae"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ef4hbEeGqX-Q",
        "outputId": "1900e49a-3f55-47df-de2f-7c7e0922290c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (12): ReLU(inplace=True)\n",
              "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (16): ReLU(inplace=True)\n",
              "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (19): ReLU(inplace=True)\n",
              "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (26): ReLU(inplace=True)\n",
              "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (32): ReLU(inplace=True)\n",
              "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (36): ReLU(inplace=True)\n",
              "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (39): ReLU(inplace=True)\n",
              "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (42): ReLU(inplace=True)\n",
              "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 0~43 까지가 feature extract 부분\n",
        "- vgg16은 1~512까지 => 1024까지는 없어 구현 필요\n",
        "  - 512까지 가져와 1024로 변환 필요 (0~34)"
      ],
      "metadata": {
        "id": "6_AxY5cIqsZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, pretrained):\n",
        "    super().__init__()\n",
        "\n",
        "    backbone = models.vgg16_bn(pretrained=pretrained).features\n",
        "    self.conv_block1 = nn.Sequential(*backbone[:6])\n",
        "    self.conv_block2 = nn.Sequential(*backbone[6:13])\n",
        "    self.conv_block3 = nn.Sequential(*backbone[13:20])\n",
        "    self.conv_block4 = nn.Sequential(*backbone[20:27])\n",
        "    self.conv_block5 = nn.Sequential(*backbone[27:34],\n",
        "                                     ConvLayer(512, 1024, kernel_size=1, padding=0)) # 채널 변환만 (kernel=1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    encode_features = []        # skip-connection\n",
        "    out = self.conv_block1(x)\n",
        "    encode_features.append(out)\n",
        "\n",
        "    out = self.conv_block2(out)\n",
        "    encode_features.append(out)\n",
        "\n",
        "    out = self.conv_block3(out)\n",
        "    encode_features.append(out)\n",
        "\n",
        "    out = self.conv_block4(out)\n",
        "    encode_features.append(out)\n",
        "\n",
        "    out = self.conv_block5(out)\n",
        "    return out, encode_features"
      ],
      "metadata": {
        "id": "bgn9Qy6aqkvD"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- encoder 잘 구현되었는지 확인"
      ],
      "metadata": {
        "id": "IYSY2Q0CsvyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(pretrained=False)\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "out, ftrs = encoder(x)"
      ],
      "metadata": {
        "id": "f2xCWgEysun5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ftr in ftrs:\n",
        "  print(ftr.shape)\n",
        "\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BcCbGaKtDeF",
        "outputId": "4a9ab3f5-0e63-4de0-d9e7-834250475ffc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 224, 224])\n",
            "torch.Size([1, 128, 112, 112])\n",
            "torch.Size([1, 256, 56, 56])\n",
            "torch.Size([1, 512, 28, 28])\n",
            "torch.Size([1, 1024, 14, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 과정 설명\n",
        "  - 224 크기의 input image가 첫번째 conv block 통과하면서 3->64 channel로 변환\n",
        "  - maxpooling 거쳐서 resolution이 반으로 줄고 (112,112), feature map은 2배로 커짐 (64 -> 128)\n",
        "  - maxpooling 거쳐서 resolution 또 반으로 줄고, feature map은 2배 증가\n",
        "  - maxpooling 거쳐서 resolution 또 반으로 줄고, feature map은 2배 증가\n",
        "- 맨 위 4가지가 skip-connection으로 전달되는 부분\n",
        "- 마지막 feature는 decoder의 input으로 들어가는 부분  "
      ],
      "metadata": {
        "id": "5ZCUS714tKn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Decoder"
      ],
      "metadata": {
        "id": "ZpyZtAl5t-88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.upconv_layer1 = UpConvLayer(in_channels=1024, out_channels=512)\n",
        "        self.conv_block1 = ConvLayer(in_channels=512+512, out_channels=512)\n",
        "\n",
        "        self.upconv_layer2 = UpConvLayer(in_channels=512, out_channels=256)\n",
        "        self.conv_block2 = ConvLayer(in_channels=256+256, out_channels=256)\n",
        "\n",
        "        self.upconv_layer3 = UpConvLayer(in_channels=256, out_channels=128)\n",
        "        self.conv_block3 = ConvLayer(in_channels=128+128, out_channels=128)\n",
        "\n",
        "        self.upconv_layer4 = UpConvLayer(in_channels=128, out_channels=64)\n",
        "        self.conv_block4 = ConvLayer(in_channels=64+64, out_channels=64)\n",
        "\n",
        "    def forward(self, x, encoder_features):\n",
        "        out = self.upconv_layer1(x)\n",
        "        out = torch.cat([out, encoder_features[-1]], dim=1)\n",
        "        out = self.conv_block1(out)\n",
        "\n",
        "        out = self.upconv_layer2(out)\n",
        "        out = torch.cat([out, encoder_features[-2]], dim=1)\n",
        "        out = self.conv_block2(out)\n",
        "\n",
        "        out = self.upconv_layer3(out)\n",
        "        out = torch.cat([out, encoder_features[-3]], dim=1)\n",
        "        out = self.conv_block3(out)\n",
        "\n",
        "        out = self.upconv_layer4(out)\n",
        "        out = torch.cat([out, encoder_features[-4]], dim=1)\n",
        "        out = self.conv_block4(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "uBmbgYk-uAmY"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(pretrained=False)\n",
        "decoder = Decoder()\n",
        "x = torch.randn(1,3,224,224)\n",
        "out, ftrs = encoder(x)\n",
        "out = decoder(out, ftrs)"
      ],
      "metadata": {
        "id": "u3SYlrSDv7Ye"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLMNu_3iwG7x",
        "outputId": "3bdcd086-0fc0-4319-9580-92bc9350748f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "  def __init__(self, num_classes, pretrained):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(pretrained=pretrained)\n",
        "    self.decoder = Decoder()\n",
        "    self.head = nn.Conv2d(64, num_classes, kernel_size=1, padding=0)  # out.shape = 64였음\n",
        "\n",
        "  def forward(self, x):\n",
        "    out, encode_features = self.encoder(x)\n",
        "    out = self.decoder(out, encode_features)\n",
        "    out = self.head(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "aV8axQRkwwop"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet(num_classes=2, pretrained=False)\n",
        "x = torch.randn(1,3,224,224)\n",
        "out = model(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqKkhQH8xcET",
        "outputId": "9be09e3a-0c1d-4431-89ed-b5243e23e7b2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLcJgewWxik8",
        "outputId": "755d18f3-cc05-4d2c-9bb2-ed02854e061b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss"
      ],
      "metadata": {
        "id": "Tf_0sjy5BbYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dice coefficient는 segmentation에 많이 사용되는 지표"
      ],
      "metadata": {
        "id": "gZVcAtRYx5t2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "vd9I6qFkxplf"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet_metric():\n",
        "    def __init__(self, num_classes):\n",
        "        self.num_classes = num_classes\n",
        "        self.CE_loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "    def __call__(self, pred, target):\n",
        "        loss1 = self.CE_loss(pred, target)\n",
        "        onehot_pred = F.one_hot(torch.argmax(pred, dim=1), num_classes=self.num_classes).permute(0, 3, 1, 2)\n",
        "        onehot_target = F.one_hot(target, num_classes=self.num_classes).permute(0, 3, 1, 2)\n",
        "        loss2 = self._get_dice_loss(onehot_pred, onehot_target)\n",
        "        loss = loss1 + loss2\n",
        "\n",
        "        dice_coefficient = self._get_batch_dice_coefficient(onehot_pred, onehot_target)\n",
        "        return loss, dice_coefficient\n",
        "\n",
        "    def _get_dice_coeffient(self, pred, target):\n",
        "        set_inter = torch.dot(pred.reshape(-1).float(), target.reshape(-1).float())\n",
        "        set_sum = pred.sum() + target.sum()\n",
        "        if set_sum.item() == 0:\n",
        "            set_sum = 2 * set_inter\n",
        "        dice_coeff = (2 * set_inter) / (set_sum + 1e-9)\n",
        "        return dice_coeff\n",
        "\n",
        "    def _get_multiclass_dice_coefficient(self, pred, target):\n",
        "        dice = 0\n",
        "        for class_index in range(1, self.num_classes):\n",
        "            dice += self._get_dice_coeffient(pred[class_index], target[class_index])\n",
        "        return dice / (self.num_classes - 1)\n",
        "\n",
        "    def _get_batch_dice_coefficient(self, pred, target):\n",
        "        num_batch = pred.shape[0]\n",
        "        dice = 0\n",
        "        for batch_index in range(num_batch):\n",
        "            dice += self._get_multiclass_dice_coefficient(pred[batch_index], target[batch_index])\n",
        "        return dice / num_batch\n",
        "\n",
        "    def _get_dice_loss(self, pred, target):\n",
        "        return 1 - self._get_batch_dice_coefficient(pred, target)"
      ],
      "metadata": {
        "id": "770UJMVwxsQX"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = build_dataloader(data_dir=data_dir2, batch_size=4)\n",
        "\n",
        "for index, batch in enumerate(dataloaders['train']):\n",
        "    images = batch[0]\n",
        "    targets = batch[1]\n",
        "    predictions = model(images)\n",
        "\n",
        "    if index ==0:\n",
        "      break"
      ],
      "metadata": {
        "id": "8Dx-Yr3t1p-R"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = UNet_metric(num_classes=2)"
      ],
      "metadata": {
        "id": "BAwayFBw4AJJ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 학습 코드"
      ],
      "metadata": {
        "id": "trNbDO013TbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(dataloaders, model, criterion, optimizer, device):\n",
        "    losses = {}\n",
        "    dice_coefficients = {}\n",
        "\n",
        "    for phase in [\"train\", \"val\"]:\n",
        "        running_loss = 0.0\n",
        "        running_dice_coeff = 0.0\n",
        "\n",
        "        if phase == \"train\":\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "\n",
        "        for index, batch in enumerate(dataloaders[phase]):\n",
        "            images = batch[0].to(device)\n",
        "            targets = batch[1].to(device)\n",
        "\n",
        "            with torch.set_grad_enabled(phase == \"train\"):\n",
        "                predictions = model(images)\n",
        "                loss, dice_coefficient = criterion(predictions, targets)\n",
        "\n",
        "            if phase == \"train\":\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_dice_coeff += dice_coefficient.item()\n",
        "\n",
        "            if index == 10: # 10 index * mini_batch 데이터수 만큼 데이터를 한정\n",
        "                break\n",
        "\n",
        "        losses[phase] = running_loss / index\n",
        "        dice_coefficients[phase] = running_dice_coeff / index\n",
        "\n",
        "    return losses, dice_coefficients"
      ],
      "metadata": {
        "id": "hZJrp-DW2NxZ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weight Initialization과 Transfer Learning 비교"
      ],
      "metadata": {
        "id": "XsZ0_hjH5UwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.He initialize"
      ],
      "metadata": {
        "id": "iXpEpxas5Zgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def He_initialization(module):\n",
        "    if isinstance(module, torch.nn.Conv2d):\n",
        "        torch.nn.init.kaiming_normal_(module.weight) # He initialization\n",
        "    elif isinstance(module, torch.nn.BatchNorm2d):\n",
        "        module.weight.data.fill_(1.0)"
      ],
      "metadata": {
        "id": "fe4EZTfA4Xd-"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 2\n",
        "BATCH_SIZE = 12\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() and is_cuda else 'cpu')\n",
        "\n",
        "dataloaders = build_dataloader(data_dir, batch_size=BATCH_SIZE)\n",
        "model = UNet(num_classes=NUM_CLASSES, pretrained=False)\n",
        "model.apply(He_initialization)\n",
        "model = model.to(DEVICE)\n",
        "criterion = UNet_metric(num_classes=NUM_CLASSES)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuM0cn7J5pqi",
        "outputId": "68a9f552-92de-4193-cc20-a042af91ed1d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "\n",
        "train_loss_def, train_dice_coefficient_def = [], []\n",
        "val_loss_def, val_dice_coefficient_def = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    losses, dice_coefficients = train_one_epoch(dataloaders, model, criterion, optimizer, DEVICE)\n",
        "    train_loss_def.append(losses[\"train\"])\n",
        "    val_loss_def.append(losses[\"val\"])\n",
        "    train_dice_coefficient_def.append(dice_coefficients[\"train\"])\n",
        "    val_dice_coefficient_def.append(dice_coefficients[\"val\"])\n",
        "\n",
        "    print(f\"{epoch}/{num_epochs} - Train loss: {losses['train']:.4f}, Val loss: {losses['val']:.4f},\" + \\\n",
        "          f\" Train dice: {dice_coefficients['train']:.4f}, Val dice: {dice_coefficients['val']:.4f}\")"
      ],
      "metadata": {
        "id": "_JMI_--o6DoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Weight Transfer pretrained on ImageNet"
      ],
      "metadata": {
        "id": "9QUcQ_5R61NE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 학습이 처음부터 선형적으로 개선"
      ],
      "metadata": {
        "id": "dVaEjk8z9US-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = build_dataloader(data_dir, batch_size=BATCH_SIZE)\n",
        "model = UNet(num_classes=NUM_CLASSES, pretrained=True)\n",
        "model = model.to(DEVICE)\n",
        "criterion = UNet_metric(num_classes=NUM_CLASSES)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "LcWTnNnm6kXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "\n",
        "train_loss_prt, train_dice_coefficient_prt = [], []\n",
        "val_loss_prt, val_dice_coefficient_prt = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    losses, dice_coefficients = train_one_epoch(dataloaders, model, criterion, optimizer, DEVICE)\n",
        "    train_loss_prt.append(losses[\"train\"])\n",
        "    val_loss_prt.append(losses[\"val\"])\n",
        "    train_dice_coefficient_prt.append(dice_coefficients[\"train\"])\n",
        "    val_dice_coefficient_prt.append(dice_coefficients[\"val\"])\n",
        "\n",
        "    print(f\"{epoch}/{num_epochs} - Train loss: {losses['train']:.4f}, Val loss: {losses['val']:.4f},\" + \\\n",
        "          f\" Train dice: {dice_coefficients['train']:.4f}, Val dice: {dice_coefficients['val']:.4f}\")"
      ],
      "metadata": {
        "id": "cb6-T5La7FEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Weight Transfer with freezing encoder layer"
      ],
      "metadata": {
        "id": "ZC3iNfWk7jhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = build_dataloader(data_dir, batch_size=BATCH_SIZE)\n",
        "model = UNet(num_classes=NUM_CLASSES, pretrained=True)\n",
        "model.encoder.reques_grad_ = False # gradient update 꺼버림\n",
        "model = model.to(DEVICE)\n",
        "criterion = UNet_metric(num_classes=NUM_CLASSES)\n",
        "# gradient update 켜진 wieght만 학습\n",
        "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "NhyJGO2E7oS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "\n",
        "train_loss_frz, train_dice_coefficient_frz = [], []\n",
        "val_loss_frz, val_dice_coefficient_frz = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    losses, dice_coefficients = train_one_epoch(dataloaders, model, criterion, optimizer, DEVICE)\n",
        "    train_loss_frz.append(losses[\"train\"])\n",
        "    val_loss_frz.append(losses[\"val\"])\n",
        "    train_dice_coefficient_frz.append(dice_coefficients[\"train\"])\n",
        "    val_dice_coefficient_frz.append(dice_coefficients[\"val\"])\n",
        "\n",
        "    print(f\"{epoch}/{num_epochs} - Train loss: {losses['train']:.4f}, Val loss: {losses['val']:.4f},\" + \\\n",
        "          f\" Train dice: {dice_coefficients['train']:.4f}, Val dice: {dice_coefficients['val']:.4f}\")"
      ],
      "metadata": {
        "id": "9s5mnzbl8JhC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}